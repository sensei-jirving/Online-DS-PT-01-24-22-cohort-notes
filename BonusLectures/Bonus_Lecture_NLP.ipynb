{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bonus Lecture: NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sensei-jirving/Online-DS-PT-01.24.22-cohort-notes/blob/main/BonusLectures/Bonus_Lecture_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thPFHP--i8Lo"
      },
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "<u>Vocab Words!</u>: \n",
        "\n",
        "**Document**: A text sample. Maybe a tweet, a sentence, a paragraph, an article, or maybe even a whole book.  However, it is represented as one row or sample.\n",
        "\n",
        "**Corpus**: A complete collection of documents.  Generally a training set.\n",
        "\n",
        "**Vocabulary** The complete list of unique words in a corpus.\n",
        "\n",
        "**Symantics**: The meaning of words.\n",
        "\n",
        "**Syntax**: How words are used grammatically.\n",
        "\n",
        "## NLP is 90% data preparation\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Load and clean data\n",
        "2. Tokenize data\n",
        "3. Remove stop words\n",
        "4. Normalize (stem or lemmatize)\n",
        "5. Vectorize\n",
        "6. Model\n",
        "7. Evaluate\n",
        "\n",
        "We will be using a 'bag of words' approach today.  The bag of words strategy does not attend to sequence or syntax, but just considers the vocabulary of the document to classify it.\n",
        "\n",
        "The final vectorized data will have one column for each word in the vocabulary of our corpus and a number in that column representing the frequency of the word in the document the row represents.\n",
        "\n",
        "In order to reduce the dimensionality of our data, we want to remove as many low-meaning or redundant words as possible.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rj2wdJGXsWh4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94e00341-fa09-4b05-8efa-73c736c3da13"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "#open zipfiles\n",
        "import zipfile\n",
        "\n",
        "#download data from online\n",
        "import requests\n",
        "\n",
        "#Natural Language Tool Kit\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3wgHEeSdrV8"
      },
      "source": [
        "# 1. Load and clean data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "bwhD8jh3jsLr",
        "outputId": "1a7bbfb5-c3d2-4adc-84a6-47ae22d0f26d"
      },
      "source": [
        "#import data\n",
        "# archive = request.get()\n",
        "data = requests.get('https://github.com/ninja-josh/image-storage/raw/main/archive.zip')\n",
        "with open('news.zip', 'wb') as f:\n",
        "  f.write(data.content)\n",
        "zf = zipfile.ZipFile('/content/news.zip')\n",
        "real_news = pd.read_csv(zf.open('True.csv'))\n",
        "fake_news = pd.read_csv(zf.open('Fake.csv'))\n",
        "real_news.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
              "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 31, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>U.S. military to accept transgender recruits o...</td>\n",
              "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 29, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
              "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 31, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
              "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 30, 2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
              "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 29, 2017</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ...                date\n",
              "0  As U.S. budget fight looms, Republicans flip t...  ...  December 31, 2017 \n",
              "1  U.S. military to accept transgender recruits o...  ...  December 29, 2017 \n",
              "2  Senior U.S. Republican senator: 'Let Mr. Muell...  ...  December 31, 2017 \n",
              "3  FBI Russia probe helped by Australian diplomat...  ...  December 30, 2017 \n",
              "4  Trump wants Postal Service to charge 'much mor...  ...  December 29, 2017 \n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "UWb7PpLWkWjH",
        "outputId": "da4bcb5f-1598-4766-9b14-096d415ac479"
      },
      "source": [
        "#add labels.  Since we are detecting fake news, we set that as class 1\n",
        "fake_news['label'] = 1\n",
        "real_news['label'] = 0\n",
        "fake_news.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
              "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
              "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
              "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 30, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
              "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 29, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
              "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
              "      <td>News</td>\n",
              "      <td>December 25, 2017</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... label\n",
              "0   Donald Trump Sends Out Embarrassing New Year’...  ...     1\n",
              "1   Drunk Bragging Trump Staffer Started Russian ...  ...     1\n",
              "2   Sheriff David Clarke Becomes An Internet Joke...  ...     1\n",
              "3   Trump Is So Obsessed He Even Has Obama’s Name...  ...     1\n",
              "4   Pope Francis Just Called Out Donald Trump Dur...  ...     1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHWtkrx8YFvd",
        "outputId": "88d9754c-4894-45fc-b691-9246cb1ac16b"
      },
      "source": [
        "#check the shape\n",
        "print(f'real_news shape: {real_news.shape}, fake_news shape: {fake_news.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "real_news shape: (21417, 5), fake_news shape: (23481, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6L8OLufYAQf",
        "outputId": "9d35bb3a-e8c7-45f4-c095-918cc965504d"
      },
      "source": [
        "#check the data types and missing values\n",
        "real_news.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 21417 entries, 0 to 21416\n",
            "Data columns (total 5 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   title    21417 non-null  object\n",
            " 1   text     21417 non-null  object\n",
            " 2   subject  21417 non-null  object\n",
            " 3   date     21417 non-null  object\n",
            " 4   label    21417 non-null  int64 \n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 836.7+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hp3nhTKPYVIm",
        "outputId": "bd4e05b6-426e-4d6e-ff70-6d7d9a8a9433"
      },
      "source": [
        "fake_news.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 23481 entries, 0 to 23480\n",
            "Data columns (total 5 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   title    23481 non-null  object\n",
            " 1   text     23481 non-null  object\n",
            " 2   subject  23481 non-null  object\n",
            " 3   date     23481 non-null  object\n",
            " 4   label    23481 non-null  int64 \n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 917.4+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FebSjMfSYnVe",
        "outputId": "5c91d97e-924a-4f73-b849-a0ac0e07c86b"
      },
      "source": [
        "#check for duplicates\n",
        "print(f'duplicates in real news: {real_news.duplicated().sum()}, duplicates in fake news: {fake_news.duplicated().sum()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "duplicates in real news: 206, duplicates in fake news: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXwQfW1EYups"
      },
      "source": [
        "real_news = real_news.drop_duplicates(subset=['text'])\n",
        "fake_news = fake_news.drop_duplicates(subset=['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPop6ljUl1hp"
      },
      "source": [
        "A quick look shows us that the real news has the location and source before the text, while the fake news does not.  We want to standardize the data, or else our model will focus on that extra formatting to determine real news, rather than the text of the article"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceXCnRfwmm6S"
      },
      "source": [
        "#split the string at the first ' - ' and then just what's after it.\n",
        "real_news['text'] = real_news['text'].str.split(' - ', n = 1)\n",
        "#some samples did not have the intro, so x[1] doesn't work.  x[-1] covers both cases.\n",
        "real_news['text'] = real_news['text'].apply(lambda x: x[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jgmi5dJZwQX",
        "outputId": "3b6496d1-11a7-473b-8cbc-886cfb77b4ce"
      },
      "source": [
        "real_news['text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        The head of a conservative Republican faction ...\n",
              "1        Transgender people will be allowed for the fir...\n",
              "2        The special counsel investigation of links bet...\n",
              "3        Trump campaign adviser George Papadopoulos tol...\n",
              "4        President Donald Trump called on the U.S. Post...\n",
              "                               ...                        \n",
              "21412    NATO allies on Tuesday welcomed President Dona...\n",
              "21413    LexisNexis, a provider of legal, regulatory an...\n",
              "21414    In the shadow of disused Soviet-era factories ...\n",
              "21415    Vatican Secretary of State Cardinal Pietro Par...\n",
              "21416    Indonesia will buy 11 Sukhoi fighter jets wort...\n",
              "Name: text, Length: 21192, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "y6FaYAdMkx8A",
        "outputId": "28c9f73a-1068-4eb5-cea9-9880155faac8"
      },
      "source": [
        "#concatenate the dataframes into one.\n",
        "df = pd.concat([real_news, fake_news], axis = 0).reset_index(drop=True)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
              "      <td>The head of a conservative Republican faction ...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>U.S. military to accept transgender recruits o...</td>\n",
              "      <td>Transgender people will be allowed for the fir...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 29, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
              "      <td>The special counsel investigation of links bet...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
              "      <td>Trump campaign adviser George Papadopoulos tol...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 30, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
              "      <td>President Donald Trump called on the U.S. Post...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 29, 2017</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... label\n",
              "0  As U.S. budget fight looms, Republicans flip t...  ...     0\n",
              "1  U.S. military to accept transgender recruits o...  ...     0\n",
              "2  Senior U.S. Republican senator: 'Let Mr. Muell...  ...     0\n",
              "3  FBI Russia probe helped by Australian diplomat...  ...     0\n",
              "4  Trump wants Postal Service to charge 'much mor...  ...     0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDheSE3mlb0X",
        "outputId": "a4467335-251c-48c1-e824-7a5698ee1d99"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 38647 entries, 0 to 38646\n",
            "Data columns (total 5 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   title    38647 non-null  object\n",
            " 1   text     38647 non-null  object\n",
            " 2   subject  38647 non-null  object\n",
            " 3   date     38647 non-null  object\n",
            " 4   label    38647 non-null  int64 \n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 1.5+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kCY4U67dI-5"
      },
      "source": [
        "# A little feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g71jl5EDdM13"
      },
      "source": [
        "We want to keep information that might be included in the title, so one one common way to do that is to just concatenate the title to the start of the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "TadKgJdPrc2O",
        "outputId": "f93bb854-cc79-4ab6-b7c7-e337fa207d7d"
      },
      "source": [
        "df['full_text'] = df['title'] + ' ' + df['text']\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "      <th>date</th>\n",
              "      <th>label</th>\n",
              "      <th>full_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
              "      <td>The head of a conservative Republican faction ...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>0</td>\n",
              "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>U.S. military to accept transgender recruits o...</td>\n",
              "      <td>Transgender people will be allowed for the fir...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 29, 2017</td>\n",
              "      <td>0</td>\n",
              "      <td>U.S. military to accept transgender recruits o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
              "      <td>The special counsel investigation of links bet...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 31, 2017</td>\n",
              "      <td>0</td>\n",
              "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
              "      <td>Trump campaign adviser George Papadopoulos tol...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 30, 2017</td>\n",
              "      <td>0</td>\n",
              "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
              "      <td>President Donald Trump called on the U.S. Post...</td>\n",
              "      <td>politicsNews</td>\n",
              "      <td>December 29, 2017</td>\n",
              "      <td>0</td>\n",
              "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38642</th>\n",
              "      <td>The White House and The Theatrics of ‘Gun Cont...</td>\n",
              "      <td>21st Century Wire says All the world s a stage...</td>\n",
              "      <td>US_News</td>\n",
              "      <td>January 7, 2016</td>\n",
              "      <td>1</td>\n",
              "      <td>The White House and The Theatrics of ‘Gun Cont...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38643</th>\n",
              "      <td>Activists or Terrorists? How Media Controls an...</td>\n",
              "      <td>Randy Johnson 21st Century WireThe majority ...</td>\n",
              "      <td>US_News</td>\n",
              "      <td>January 7, 2016</td>\n",
              "      <td>1</td>\n",
              "      <td>Activists or Terrorists? How Media Controls an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38644</th>\n",
              "      <td>BOILER ROOM – No Surrender, No Retreat, Heads ...</td>\n",
              "      <td>Tune in to the Alternate Current Radio Network...</td>\n",
              "      <td>US_News</td>\n",
              "      <td>January 6, 2016</td>\n",
              "      <td>1</td>\n",
              "      <td>BOILER ROOM – No Surrender, No Retreat, Heads ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38645</th>\n",
              "      <td>Federal Showdown Looms in Oregon After BLM Abu...</td>\n",
              "      <td>21st Century Wire says A new front has just op...</td>\n",
              "      <td>US_News</td>\n",
              "      <td>January 4, 2016</td>\n",
              "      <td>1</td>\n",
              "      <td>Federal Showdown Looms in Oregon After BLM Abu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38646</th>\n",
              "      <td>A Troubled King: Chicago’s Rahm Emanuel Desper...</td>\n",
              "      <td>21st Century Wire says It s not that far away....</td>\n",
              "      <td>US_News</td>\n",
              "      <td>January 2, 2016</td>\n",
              "      <td>1</td>\n",
              "      <td>A Troubled King: Chicago’s Rahm Emanuel Desper...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>38647 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   title  ...                                          full_text\n",
              "0      As U.S. budget fight looms, Republicans flip t...  ...  As U.S. budget fight looms, Republicans flip t...\n",
              "1      U.S. military to accept transgender recruits o...  ...  U.S. military to accept transgender recruits o...\n",
              "2      Senior U.S. Republican senator: 'Let Mr. Muell...  ...  Senior U.S. Republican senator: 'Let Mr. Muell...\n",
              "3      FBI Russia probe helped by Australian diplomat...  ...  FBI Russia probe helped by Australian diplomat...\n",
              "4      Trump wants Postal Service to charge 'much mor...  ...  Trump wants Postal Service to charge 'much mor...\n",
              "...                                                  ...  ...                                                ...\n",
              "38642  The White House and The Theatrics of ‘Gun Cont...  ...  The White House and The Theatrics of ‘Gun Cont...\n",
              "38643  Activists or Terrorists? How Media Controls an...  ...  Activists or Terrorists? How Media Controls an...\n",
              "38644  BOILER ROOM – No Surrender, No Retreat, Heads ...  ...  BOILER ROOM – No Surrender, No Retreat, Heads ...\n",
              "38645  Federal Showdown Looms in Oregon After BLM Abu...  ...  Federal Showdown Looms in Oregon After BLM Abu...\n",
              "38646  A Troubled King: Chicago’s Rahm Emanuel Desper...  ...  A Troubled King: Chicago’s Rahm Emanuel Desper...\n",
              "\n",
              "[38647 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJEXW-mSdmw5"
      },
      "source": [
        "# 2. Tokenizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e7Yq5Ojd4Jw"
      },
      "source": [
        "We want to isolate each word into a separate token so we can remove stop words, punctuation, capitals, and other special characters.  Our model will represent words like 'Horse', 'horse.', and 'horse' as different words.  We want them to be considered the same.\n",
        "\n",
        "I've also decided that I want to remove all numbers, punctuation, and special characters.  I'll use regex to only return letters and whitespace from the original document before splitting it into tokens with str.split().\n",
        "\n",
        "I highly recommend using [Regexr.com](https://regexr.com/) to practice your regular expression and experiment to get the right expression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idu2W40Rdeba"
      },
      "source": [
        "import re\n",
        "#tokenize text, lower case, remove all symbols except letters\n",
        "def clean_text(doc):\n",
        "  doc = doc.lower()\n",
        "  doc = re.sub(f\"[^a-z\\s]+\", '', doc)\n",
        "  tokens = doc.split()\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwi6EZnAmNZL",
        "outputId": "646ac1ce-a300-444d-d36e-95f5b02044b3"
      },
      "source": [
        "#series.apply() is another important tool for NLP.  \n",
        "df['tokens'] = df['full_text'].apply(clean_text)\n",
        "df['tokens']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        [as, us, budget, fight, looms, republicans, fl...\n",
              "1        [us, military, to, accept, transgender, recrui...\n",
              "2        [senior, us, republican, senator, let, mr, mue...\n",
              "3        [fbi, russia, probe, helped, by, australian, d...\n",
              "4        [trump, wants, postal, service, to, charge, mu...\n",
              "                               ...                        \n",
              "38642    [the, white, house, and, the, theatrics, of, g...\n",
              "38643    [activists, or, terrorists, how, media, contro...\n",
              "38644    [boiler, room, no, surrender, no, retreat, hea...\n",
              "38645    [federal, showdown, looms, in, oregon, after, ...\n",
              "38646    [a, troubled, king, chicagos, rahm, emanuel, d...\n",
              "Name: tokens, Length: 38647, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myVAnAGIpOe_"
      },
      "source": [
        "# 3. Remove Stopwords\n",
        "\n",
        "We can use list comprehension to return a list of the words in each list of tokens that are not present in our list of stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynHwYyFZp9et",
        "outputId": "15019bdf-93db-4f7c-f337-bc37bc9c803a"
      },
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "stopwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MRvplNapUBd",
        "outputId": "ad894f5f-43fb-431f-c802-733f5198eada"
      },
      "source": [
        "#removes stop words from a list of tokens\n",
        "def remove_stops(token_list):\n",
        "  #this is list comprehension below.  We haven't learned it in the curriculum\n",
        "  #but you can essentially put a loop in a list format to generate that list.\n",
        "  #this is another very common tool for NLP.\n",
        "  no_stops = [word for word in token_list if word not in stopwords]\n",
        "  return no_stops\n",
        "\n",
        "df['no_stop'] = df['tokens'].apply(remove_stops)\n",
        "df['no_stop']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        [us, budget, fight, looms, republicans, flip, ...\n",
              "1        [us, military, accept, transgender, recruits, ...\n",
              "2        [senior, us, republican, senator, let, mr, mue...\n",
              "3        [fbi, russia, probe, helped, australian, diplo...\n",
              "4        [trump, wants, postal, service, charge, much, ...\n",
              "                               ...                        \n",
              "38642    [white, house, theatrics, gun, control, st, ce...\n",
              "38643    [activists, terrorists, media, controls, dicta...\n",
              "38644    [boiler, room, surrender, retreat, heads, roll...\n",
              "38645    [federal, showdown, looms, oregon, blm, abuse,...\n",
              "38646    [troubled, king, chicagos, rahm, emanuel, desp...\n",
              "Name: no_stop, Length: 38647, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqzrHz0gncJ0"
      },
      "source": [
        "# 4. Lemmatizing\n",
        "\n",
        "We can further reduce our vocabulary (dimensionality of our dataset) by lemmatizing.  This transforms words into their root.  Run and runs are transformed into 'run' since they have the same semantic meaning with only syntactic differences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-J0ioxF8fUKF",
        "outputId": "b22d056d-bf1e-4dd4-b79c-97f65f7d30ff"
      },
      "source": [
        "#lemmatizer from the nltk package\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "lemmatizer.lemmatize('runs')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'run'"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thphgF1-kaXg",
        "outputId": "b963e06e-5b63-43f6-af43-df943c1b3154"
      },
      "source": [
        "#function to lemmatize a list of tokens\n",
        "def lemmatize_tokens(token_list):\n",
        "  lemmas = [lemmatizer.lemmatize(token) for token in token_list]\n",
        "  return lemmas\n",
        "\n",
        "df['lemmas'] = df['no_stop'].apply(lemmatize_tokens)\n",
        "df['lemmas']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        [u, budget, fight, loom, republican, flip, fis...\n",
              "1        [u, military, accept, transgender, recruit, mo...\n",
              "2        [senior, u, republican, senator, let, mr, muel...\n",
              "3        [fbi, russia, probe, helped, australian, diplo...\n",
              "4        [trump, want, postal, service, charge, much, a...\n",
              "                               ...                        \n",
              "38642    [white, house, theatrics, gun, control, st, ce...\n",
              "38643    [activist, terrorist, medium, control, dictate...\n",
              "38644    [boiler, room, surrender, retreat, head, roll,...\n",
              "38645    [federal, showdown, loom, oregon, blm, abuse, ...\n",
              "38646    [troubled, king, chicago, rahm, emanuel, despe...\n",
              "Name: lemmas, Length: 38647, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbp2zolIq8jL"
      },
      "source": [
        "# 5. Vectorizing\n",
        "\n",
        "The final step in the data preparation is to transform our list of token words into vectors of floats or integers.  We will use sklearn tools for this.\n",
        "\n",
        "## Count Vectorizing\n",
        "The simplest form of bag of words vectorization is count vectorizing.  Each column of our data will be a word in the vocabulary of our training corpus.  Each value for each of our matrix will the number of times that word appears in a given document.  Obviously the vast majority of values will be zero.\n",
        "\n",
        "We will be using an sklearn transformer for this task.\n",
        "\n",
        "The columns should only represent words in our training corpus, since our model should not know anything about our testing data.  Therefor we need to perform our validation split before vectorizing.\n",
        "\n",
        "NOTE: The sklearn vectorizers expect one string, not a list of tokens.  We will need to rejoin our list of tokens back into one long string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vApIAUA9tC5s",
        "outputId": "f9003e0a-ebbc-4e56-fc18-61ae0446d81e"
      },
      "source": [
        "#combine token lists back into one string\n",
        "df['lemmas'].str.join(' ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        u budget fight loom republican flip fiscal scr...\n",
              "1        u military accept transgender recruit monday p...\n",
              "2        senior u republican senator let mr mueller job...\n",
              "3        fbi russia probe helped australian diplomat ti...\n",
              "4        trump want postal service charge much amazon s...\n",
              "                               ...                        \n",
              "38642    white house theatrics gun control st century w...\n",
              "38643    activist terrorist medium control dictate narr...\n",
              "38644    boiler room surrender retreat head roll ep tun...\n",
              "38645    federal showdown loom oregon blm abuse local r...\n",
              "38646    troubled king chicago rahm emanuel desperate s...\n",
              "Name: lemmas, Length: 38647, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmHjda8StCps"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_OH56Fsqeab"
      },
      "source": [
        "#train/test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df['lemmas'].str.join(' ')\n",
        "y = df['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxZiUnlpsaxs",
        "outputId": "a41c5659-276d-42b5-f454-d2eb4b301300"
      },
      "source": [
        "#vectorize the strings\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(X_train)\n",
        "\n",
        "X_train_vec = vectorizer.transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "X_train_vec.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28985, 170807)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNQz-NEptloQ"
      },
      "source": [
        "As you can see, the shape of X_train_vec has exploded.  It now has 170,119 columns along with the nearly 30k rows.  That represents 4,930,899,215 values.  This would not normally fit in memory, except that the vectorizer has compressed the data.  Remember we said it's mostly zeros?  \n",
        "\n",
        "This new type of array is called a 'sparse array' and sklearn models and handle them just fine.  They compress out all of the zeros and just store the non-zero elements along with a tag for where in the matrix they belong."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyp5P9gJsxhk",
        "outputId": "68885fb2-a84b-4b20-d117-6b775f983fc7"
      },
      "source": [
        "type(X_train_vec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "scipy.sparse.csr.csr_matrix"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEnJBJF7uUxh"
      },
      "source": [
        "# 6. Modeling\n",
        "\n",
        "Modeling vectorized text is actually just like modeling any other tabular data.  Let's try our good ol' logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO4kVQMztpGt",
        "outputId": "fe2f14bc-036e-483b-e5ea-0f7be2251f93"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "logreg.fit(X_train_vec, y_train)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCTdHq7butfg",
        "outputId": "fda8de55-7a61-4999-924e-f839329588d5"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_preds = logreg.predict(X_test_vec)\n",
        "accuracy = accuracy_score(y_test, y_preds)\n",
        "print(f'The accuracy of our logistic regression model is: {accuracy}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of our logistic regression model is: 0.9819913061477955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAefsTqcvOgG"
      },
      "source": [
        "# 7. Evaluate\n",
        "\n",
        "Let's take a quick look at a confusion matrix to see which class was harder to predict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "nz0GsOcwvVxC",
        "outputId": "c87b6bf8-bddf-4807-b907-aba3d7587ba2"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from seaborn import heatmap\n",
        "conf_mat = confusion_matrix(y_test, y_preds, normalize='true')\n",
        "\n",
        "heatmap(conf_mat, annot=True, cmap='Greens', \n",
        "        xticklabels=['True', 'Fake'],\n",
        "        yticklabels=['True', 'Fake'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fda92f41e90>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_Db92ZSxRyh"
      },
      "source": [
        "# Next Steps\n",
        "\n",
        "NLP goes MUCH deeper than this.  Even with a bag of words approach there are many more steps we could take:\n",
        "\n",
        "1. Make a frequency count of words and remove words that appear only once or that are very very common.\n",
        "2. Use TF-IDF vectorizing.  This is a slightly more complext vectorizing strategy.  Each column is still a word in the vocabulary, but the value is a float that represents how specific that word is to that document compared to other documents in the corpus.  Read more [here](https://towardsdatascience.com/text-vectorization-term-frequency-inverse-document-frequency-tfidf-5a3f9604da6d).\n",
        "3. Try stemming, removing or more or fewer stop words, exploring the data for more irregularities.\n",
        "4. Add more feature engineering.  We could add extra columns for the length of each document (since that is lost in the vectorizing process), the length of the title, the average length of the words in each document, the date from the date column, or some other meta-data about the text.\n",
        "5. Apply other vectorization strategies that preserve sequence such as word embeddings or integer vectorization\n",
        "\n",
        "There is also a large field of work using deep learning models, recurrent neural networks, to classify text.  [You can read more about this on my blog](https://towardsdatascience.com/pretrained-word-embeddings-using-spacy-and-keras-textvectorization-ef75ecd56360)\n",
        "\n",
        "## Sequence to Sequence\n",
        "\n",
        "Another large field of NLP is transformers and sequence to sequence NLP.  In this approach a model returns a text sequence rather than a single label.  This is useful in translation machines, chatbots, or voice controlled AI like Siri or Alexa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4QNZALzzbOH"
      },
      "source": [
        "# Final Note: Spoiler\n",
        "\n",
        "With the exception of lemmatizing, the sklearn CountVectorizer can do all of these steps automatically, including removing punctuation, lower casing, and removing stop words.  You can also pass it a custom function if you want to do other prepreprocessing, such as lemmatizing.  Check out the documentation for the class for more information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMoX1o6nvpi7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}